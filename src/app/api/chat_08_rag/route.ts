/**
 * ===============================================
 * API Route ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Chat (Gemini RAG Version)
 * ===============================================
 */

import { NextRequest } from 'next/server'
import { getDatabase } from '@/src/lib/database'

// LangChain & AI SDK Imports
// üîÑ CHANGED: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å OpenAI ‡πÄ‡∏õ‡πá‡∏ô Google GenAI
import { ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings } from "@langchain/google-genai";
import { ChatPromptTemplate, MessagesPlaceholder } from '@langchain/core/prompts'
import { toUIMessageStream } from '@ai-sdk/langchain'
import { createUIMessageStreamResponse, UIMessage } from 'ai'
import { PostgresChatMessageHistory } from '@langchain/community/stores/message/postgres'
import { BaseMessage, AIMessage, HumanMessage, SystemMessage, MessageContent } from '@langchain/core/messages'
import { trimMessages } from '@langchain/core/messages'
import { StringOutputParser } from '@langchain/core/output_parsers'
// ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: tiktoken ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö token ‡∏Ç‡∏≠‡∏á OpenAI ‡πÅ‡∏ï‡πà‡πÄ‡∏£‡∏≤‡∏¢‡∏±‡∏á‡πÉ‡∏ä‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ
import { encodingForModel } from '@langchain/core/utils/tiktoken' 
import { createClient } from '@supabase/supabase-js'
// ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô Import
import { HarmCategory, HarmBlockThreshold } from "@google/generative-ai";

// Imports for Vector Search (Document RAG)
import { SupabaseVectorStore } from "@langchain/community/vectorstores/supabase"
import { CacheBackedEmbeddings } from "@langchain/classic/embeddings/cache_backed";
import { InMemoryStore } from "@langchain/core/stores"
import { TaskType } from "@google/generative-ai";

export const dynamic = 'force-dynamic'
export const maxDuration = 30

const pool = getDatabase()

const supabase = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL!,
  process.env.NEXT_PUBLIC_SUPABASE_PUBLISHABLE_OR_ANON_KEY!
)

// ===============================================
// üîÑ MODIFIED: ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡∏î‡πâ‡∏ß‡∏¢ Google Embeddings
// ===============================================
async function createVectorStore() {
  // ‡πÉ‡∏ä‡πâ Google Embeddings (text-embedding-004)
  const baseEmbeddings = new GoogleGenerativeAIEmbeddings({
    model: process.env.GOOGLE_EMBEDDING_MODEL_NAME || "text-embedding-004",
    taskType: TaskType.RETRIEVAL_DOCUMENT, // ‡∏£‡∏∞‡∏ö‡∏∏ Task Type ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
    // Google embeddings ‡∏õ‡∏Å‡∏ï‡∏¥‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô 768 dimensions 
    // ‡∏´‡∏≤‡∏Å Table ‡πÉ‡∏ô Supabase ‡∏ï‡∏±‡πâ‡∏á‡πÑ‡∏ß‡πâ 1536 ‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ Database ‡∏´‡∏£‡∏∑‡∏≠ Re-create table
  });

  const cacheStore = new InMemoryStore();
  const embeddings = CacheBackedEmbeddings.fromBytesStore(
    baseEmbeddings,
    cacheStore,
    {
      namespace: "rag_embeddings_gemini" // ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô namespace ‡πÄ‡∏•‡πá‡∏Å‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏µ‡∏Å‡∏±‡∏ö cache ‡πÄ‡∏î‡∏¥‡∏°
    }
  );

  return new SupabaseVectorStore(embeddings, {
    client: supabase,
    tableName: 'documents',
    queryName: 'match_documents'
  });
}

// ===============================================
// ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG (Vector Search)
// ===============================================
async function searchDocuments(query: string, limit: number = 5) {
  try {
    console.log(`üîß Searching documents (Gemini) with query="${query}", limit=${limit}`);
    
    const vectorStore = await createVectorStore();
    
    const results = await vectorStore.similaritySearchWithScore(query, limit);
    
    if (!results || results.length === 0) {
      return `‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö "${query}" ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö`;
    }
    
    console.log(`‚úÖ ‡∏û‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: ${results.length} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£`);
    
    const documents = results.map(([doc, score]) => {
      const filename = doc.metadata?.filename || '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå';
      const type = doc.metadata?.type || '‡πÑ‡∏°‡πà‡∏ó‡∏£‡∏≤‡∏ö‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó';
      return `‡πÑ‡∏ü‡∏•‡πå: ${filename} (${type.toUpperCase()})
‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: ${doc.pageContent}
‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: ${(score * 100).toFixed(1)}%`; // Note: Score ‡∏Ç‡∏≠‡∏á Google ‡∏≠‡∏≤‡∏à‡∏ï‡πà‡∏≤‡∏á scale ‡∏Å‡∏±‡∏ö OpenAI
    }).join('\n\n---\n\n');
    
    return documents;
  } catch (e: unknown) {
    const errorMessage = e instanceof Error ? e.message : String(e);
    console.log('‚ùå Search error:', errorMessage);
    
    if (errorMessage.includes('connection') || 
        errorMessage.includes('network') || 
        errorMessage.includes('timeout')) {
      throw new Error('‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ');
    }
    
    throw new Error(`‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£: ${errorMessage}`);
  }
}

// ===============================================
// ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ô‡∏±‡∏ö Token (Tiktoken Proxy)
// ===============================================
// ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡πÅ‡∏°‡πâ‡πÄ‡∏£‡∏≤‡πÉ‡∏ä‡πâ Gemini ‡πÅ‡∏ï‡πà‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Tiktoken (‡∏Ç‡∏≠‡∏á GPT-4) 
// ‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡πá‡∏ß‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥ Trimming
// ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏£‡∏≤‡πÅ‡∏Ñ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πä‡∏∞ 100% ‡∏ï‡∏≤‡∏°‡πÇ‡∏°‡πÄ‡∏î‡∏• Gemini

type Encoding = {
  encode: (text: string) => number[]
  free?: () => void
}

let encPromise: Promise<Encoding> | undefined

async function getEncoder(): Promise<Encoding> {
  if (!encPromise) {
    // ‡πÉ‡∏ä‡πâ gpt-4o ‡πÄ‡∏õ‡πá‡∏ô standard reference ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß
    encPromise = encodingForModel("gpt-4o").catch(() =>
      encodingForModel("gpt-4")
    )
  }
  return encPromise
}

async function strTokenCounter(content: MessageContent): Promise<number> {
  const enc = await getEncoder()
  if (typeof content === 'string') return enc.encode(content).length
  if (Array.isArray(content)) {
    return enc.encode(
      content.map(p => (p.type === 'text' ? p.text : JSON.stringify(p))).join(' ')
    ).length
  }
  return enc.encode(String(content ?? '')).length
}

async function tiktokenCounter(messages: BaseMessage[]): Promise<number> {
  let total = 0
  for (const m of messages) {
    const role =
      m instanceof HumanMessage ? 'user'
        : m instanceof AIMessage ? 'assistant'
        : m instanceof SystemMessage ? 'system'
        : 'unknown'
    total += await strTokenCounter(role)
    total += await strTokenCounter(m.content)
  }
  return total
}

// ===============================================
// POST API: ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡πÅ‡∏ö‡∏ö Stream
// ===============================================
export async function POST(req: NextRequest) {
  try {
    const { messages, sessionId, userId }: {
      messages: UIMessage[]
      sessionId?: string
      userId?: string
    } = await req.json()

    // --- Step 2: Session Management (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
    let currentSessionId = sessionId
    if (!currentSessionId) {
      const client = await pool.connect()
      try {
        const firstMessage = messages.find(m => m.role === 'user')
        let title = 'New Chat'
        if (firstMessage && Array.isArray(firstMessage.parts) && firstMessage.parts.length > 0) {
          const textPart = firstMessage.parts.find(p => p.type === 'text')
          if (textPart && typeof textPart.text === 'string') {
            title = textPart.text.slice(0, 50) + (textPart.text.length > 50 ? '...' : '')
          }
        }
        
        if (!userId) throw new Error('User ID is required')
        const result = await client.query(
          'INSERT INTO chat_sessions (title, user_id) VALUES ($1, $2) RETURNING id',
          [title, userId]
        )
        currentSessionId = result.rows[0].id
      } finally {
        client.release()
      }
    }

    // --- Step 3: Load Summary (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
    const clientForSummary = await pool.connect()
    let persistedSummary = ''
    try {
      const r = await clientForSummary.query(
        'SELECT summary FROM chat_sessions WHERE id = $1 LIMIT 1',
        [currentSessionId]
      )
      persistedSummary = r.rows?.[0]?.summary ?? ''
    } finally {
      clientForSummary.release()
    }

    // ===============================================
    // üîÑ MODIFIED Step 4: ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ AI Model (Gemini)
    // ===============================================
    const model = new ChatGoogleGenerativeAI({
      model: process.env.GOOGLE_MODEL_NAME || "gemini-2.5-flash",
      temperature: 0.1,
      maxOutputTokens: 8192, // Gemini ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö Output ‡∏¢‡∏≤‡∏ß‡∏Å‡∏ß‡πà‡∏≤
      streaming: true,
        apiKey: process.env.GOOGLE_API_KEY,
        safetySettings: [
            {
            category: HarmCategory.HARM_CATEGORY_HARASSMENT,
            threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
            category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
            threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
            category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
            threshold: HarmBlockThreshold.BLOCK_NONE,
            },
            {
            category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
            threshold: HarmBlockThreshold.BLOCK_NONE,
            },
        ],
    })

    // --- Step 5: History (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
    const messageHistory = new PostgresChatMessageHistory({
      sessionId: currentSessionId!,
      tableName: 'chat_messages',
      pool: pool
    })

    const fullHistory = await messageHistory.getMessages()
    
    // --- Step 6: Last User Message (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
    const lastUserMessage = messages.filter(m => m.role === 'user').pop()
    let input = ''
    if (lastUserMessage && Array.isArray(lastUserMessage.parts) && lastUserMessage.parts.length > 0) {
      const textPart = lastUserMessage.parts.find(p => p.type === 'text')
      if (textPart) input = textPart.text
    }
    if (!input) return new Response('No valid user input found.', { status: 400 })

    // --- Step 7: Trimming & Summary (‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡πÄ‡∏î‡∏¥‡∏°) ---
    let recentWindowWithoutCurrentInput: BaseMessage[] = []
    let overflowSummary = ''
    
    if (sessionId && fullHistory.length > 0) {
      // ‡πÉ‡∏ä‡πâ tiktokenCounter (‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô proxy) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
      const trimmedWindow = await trimMessages(fullHistory, {
        maxTokens: 3000, // Gemini Flash ‡∏£‡∏±‡∏ö Context ‡πÑ‡∏î‡πâ‡πÄ‡∏¢‡∏≠‡∏∞ ‡πÄ‡∏û‡∏¥‡πà‡∏° limit ‡πÑ‡∏î‡πâ
        strategy: 'last',
        tokenCounter: tiktokenCounter
      })

      recentWindowWithoutCurrentInput = trimmedWindow.filter(msg => {
        if (msg instanceof HumanMessage && msg.content === input) return false
        return true
      })

      const windowSet = new Set(trimmedWindow)
      const overflow = fullHistory.filter(m => !windowSet.has(m))
      if (overflow.length > 0) {
        const summarizerPrompt = ChatPromptTemplate.fromMessages([
          ['system', '‡∏™‡∏£‡∏∏‡∏õ‡∏ö‡∏ó‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç'],
          ['human', '‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:\n\n{history}']
        ])
        const summarizer = summarizerPrompt.pipe(model).pipe(new StringOutputParser())
        const historyText = overflow
          .map(m => {
            if (m instanceof HumanMessage) return `‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: ${m.content}`
            if (m instanceof AIMessage) return `‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢: ${m.content}`
            return `‡∏£‡∏∞‡∏ö‡∏ö: ${String(m.content)}`
          })
          .join('\n')
        try {
          overflowSummary = await summarizer.invoke({ history: historyText })
        } catch (e) {
          console.warn('overflow summary failed', e)
        }
      }
    }

    const summaryForThisTurn = [persistedSummary, overflowSummary].filter(Boolean).join('\n')

    // ===============================================
    // Step 8: RAG Chain (Prompt ‡πÄ‡∏î‡∏¥‡∏° ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÄ‡∏•‡∏¢)
    // ===============================================
    const ragPrompt = ChatPromptTemplate.fromMessages([
      ['system', `‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞‡∏ó‡∏µ‡πà‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 
      
      ‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
      
      **‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:**
      - ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏°‡∏≤‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
      - ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡∏ö‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
      - ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏î‡∏≤‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÄ‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
      - ‡∏ï‡∏≠‡∏ö‡∏î‡πâ‡∏ß‡∏¢‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô
      
      ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏Ñ‡∏∑‡∏≠: {summary}
      
      ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:
      {context}`],
      new MessagesPlaceholder('chat_history'),
      ['human', '{input}']
    ])

    const ragChain = ragPrompt.pipe(model).pipe(new StringOutputParser())

    // --- Step 9: Search & Stream ---
    let documentContext = '';
    try {
      documentContext = await searchDocuments(input, 3);
    } catch (error) {
      console.warn('‚ö†Ô∏è Search Error:', error instanceof Error ? error.message : String(error));
      documentContext = '‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ';
    }

    const chatHistoryForChain = [...recentWindowWithoutCurrentInput];
    // if (summaryForThisTurn) {
    //     chatHistoryForChain.unshift(new SystemMessage(summaryForThisTurn));
    // }

    const stream = await ragChain.stream({
        input: input,
        chat_history: chatHistoryForChain,
        summary: summaryForThisTurn,
        context: documentContext
    });

    // --- Step 10: Save User Message ---
    let canSaveToDatabase = true
    try {
      await messageHistory.addUserMessage(input)
    } catch (e) {
      console.warn('‚ö†Ô∏è Save user msg failed:', e)
      canSaveToDatabase = false
    }
    
    // --- Step 11: Handle Stream ---
    let assistantText = ''
    let hasSearchError = false
    
    const readable = new ReadableStream({
      async start(controller) {
        try {
        console.log('üöÄ Start Streaming response...');
          for await (const chunk of stream) {
            if (typeof chunk === 'string') {
              assistantText += chunk;
              if (chunk.includes('‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ') || 
                  assistantText.includes('‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ')) {
                hasSearchError = true;
                const friendlyMessage = '‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Ç‡∏ì‡∏∞‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÑ‡∏î‡πâ ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏•‡∏≠‡∏á‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÉ‡∏ô‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á';
                controller.enqueue(friendlyMessage);
                assistantText = friendlyMessage;
              } else {
                controller.enqueue(chunk);
              }
            }
          }

          if (!assistantText) {
             console.warn('‚ö†Ô∏è ‡∏à‡∏ö Stream ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Empty Response)');
          } else {
             console.log('‚úÖ Stream Finished. Total length:', assistantText.length);
          }
          
          // --- Step 12 & 13: Save AI Message & Update Summary ---
          if (assistantText && !hasSearchError && canSaveToDatabase) {
            try {
              await messageHistory.addMessage(new AIMessage(assistantText))
              
              const summarizerPrompt2 = ChatPromptTemplate.fromMessages([
                ['system', '‡∏£‡∏ß‡∏°‡∏™‡∏≤‡∏£‡∏∞‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö'],
                ['human', '‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏î‡∏¥‡∏°:\n{old}\n\n‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà:\n{delta}\n\n‡∏ä‡πà‡∏ß‡∏¢‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô']
              ])
              const summarizer2 = summarizerPrompt2.pipe(model).pipe(new StringOutputParser())
              const updatedSummary = await summarizer2.invoke({
                old: persistedSummary || '‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤',
                delta: [overflowSummary, `‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: ${input}`, `‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢: ${assistantText}`].filter(Boolean).join('\n')
              })
              
              const clientUpdate = await pool.connect()
              try {
                await clientUpdate.query(
                  'UPDATE chat_sessions SET summary = $1 WHERE id = $2',
                  [updatedSummary, currentSessionId]
                )
              } finally {
                clientUpdate.release()
              }
            } catch (e) {
              console.warn('Update summary failed', e)
            }
          }
          
          controller.close()
        } catch (error) {
          controller.error(error)
        }
      }
    })

    // --- Step 14: Response ---
    return createUIMessageStreamResponse({
      stream: toUIMessageStream(readable),
      headers: currentSessionId ? { 'x-session-id': currentSessionId } : undefined
    })
  } catch (error) {
    console.error('API Error:', error)
    return new Response(
      JSON.stringify({
        error: 'An error occurred',
        details: error instanceof Error ? error.message : 'Unknown error'
      }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    )
  }
}

// ===============================================
// GET API: (‡∏Ñ‡∏á‡πÄ‡∏î‡∏¥‡∏° ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ)
// ===============================================
export async function GET(req: NextRequest) {
  try {
    // ===============================================
    // Step 1: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Session ID ‡∏à‡∏≤‡∏Å URL Parameters
    // ===============================================
    const { searchParams } = new URL(req.url)
    const sessionId = searchParams.get('sessionId')
    if (!sessionId) {
      return new Response(
        JSON.stringify({ error: 'Session ID is required' }),
        { status: 400, headers: { 'Content-Type': 'application/json' } }
      )
    }

    // ===============================================
    // Step 2: Query ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏à‡∏≤‡∏Å‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
    // ===============================================
    const client = await pool.connect()
    try {
      const result = await client.query(
        `SELECT message, message->>'type' as message_type, created_at
         FROM chat_messages 
         WHERE session_id = $1 
         ORDER BY created_at ASC`,
        [sessionId]
      )
      
      // ===============================================
      // Step 3: ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà UI ‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
      // ===============================================
      const messages = result.rows.map((row, i) => {
        const data = row.message
        let role = 'user'
        if (row.message_type === 'ai') role = 'assistant'
        else if (row.message_type === 'human') role = 'user'
        return {
          id: `history-${i}`,
          role,
          content: data.content || data.text || data.message || '',
          createdAt: row.created_at
        }
      })
      
      // ===============================================
      // Step 4: ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏•‡∏±‡∏ö
      // ===============================================
      return new Response(JSON.stringify({ messages }), {
        status: 200,
        headers: { 'Content-Type': 'application/json' }
      })
    } finally {
      client.release()
    }
  } catch (error) {
    console.error('Error fetching messages:', error)
    return new Response(
      JSON.stringify({
        error: 'Failed to fetch messages',
        details: error instanceof Error ? error.message : 'Unknown error'
      }),
      { status: 500, headers: { 'Content-Type': 'application/json' } }
    )
  }
}